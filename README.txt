Minimum Perplexity cases:(<20)
`` .
P.S. .
Dutton .
A5 .
Norma .
Unglazed .
Ronnel .
Sulfaquinoxaline . 
2.1.6 .
Refunds .
5.4 .
6.4 .
Status-roles .
Syllabification .
2-2 .
2-3 .
2-4 .
2-5 .
7-3 .
7-4 .
7-5 .
7-6 .
Keerist .
Goddammit .
Deportees .
`` No-o-o .
`` huh-uh .
`` Gracias .
Wow .
Pugh .
`` M-m-m .
31


Maximum Perplexity Cases: (>35000)
Oslo
Plainfield
Hollywood
Hit 
Deterrent
Undergraduates
Vacancy
Broadway
Nightclubs
Confrontation
Americana
Festivals
Movies
Frame
Somersaults
Communications
Eligibility
Repayment
Uniconer
Theater
Athletics
Secretion
Subsystems
Subjects 
Poland
Meats
Poultry
Seafood
Molding
Principle
Procedure
Thirty-four
Tuesday
Thirty-six

I found that the sentences with the lowest perplexity ended up being 'sentences' or simply just two elements of text  
made up of a word followed by a period.  I'm guessing these sentences were read in as the last line of a text in a
given paragraph directly after a newline and were so predictable because the next/last word was a period, which all 
sentences in english end with.  Some common themes among these sentences were numerical scores and decimal numbers.

I found that the 'sentences' with the highest perplexity were also simlarly not complete english sentences but read in 
from the corpus as a sentence but only containing 1 word.  These words all seemed to be nouns with no common theme, 
though you might be able to make a case for the Meats, Poultry, and Seafood all being present within these unpredictable
sentences.  I think these 'sentences' had such high perplexity values because they didn't follow any traditional sentence
format, there are no periods, there is no subject predicate structure or anything that constitutes a real sentence.

If I were to do this again I would try and clean these non-sentences out of the corpus and try find which sentences truly had
the highest and lowest values that still follow basic sentence structure rules.



Feedback:
How long did you spend on the assignment?

15-20 hours

What did you learn by doing this assignment?

I learned how basic language models function by creating and finalizing a vocabulary, and train the language models
so that they can be used on test data to see how predictable a sentence can be given a dataset of sentences.  I also
learned various smoothing methods that can be performed on our ngrams to implement probability estimates for contexts.

Briefly describe a Computational Text Analysis/Text as Data research question 
where language modeling would be useful (keep this answer to a maximum of 3 sentences).

